{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wps0/deep4life/blob/main/Project04_CellSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ82esH_1lEq"
      },
      "source": [
        "## Project : Cell image segmentation projects\n",
        "\n",
        "Contact: Elena Casiraghi (University Milano elena.casiraghi@unimi.it)\n",
        "\n",
        "Cell segmentation is usually the first step for downstream single-cell analysis in microscopy image-based biology and biomedical research. Deep learning has been widely used for cell-image segmentation.\n",
        "The CellSeg competition aims to benchmark cell segmentation methods that could be applied to various microscopy images across multiple imaging platforms and tissue types for cell Segmentation. The  Dataset challenge organizers provide both labeled images and unlabeled ones.\n",
        "The “2018 Data Science Bowl” Kaggle competition provides cell images and their masks for training cell/nuclei segmentation models.\n",
        "\n",
        "In 2022 another [Cell Segmentation challenge was proposed at Neurips](https://neurips22-cellseg.grand-challenge.org/).\n",
        "For interested readers, the competition proceeding has been published on [PMLR](https://proceedings.mlr.press/v212/)\n",
        "\n",
        "### Project Description\n",
        "\n",
        "In the field of (bio-medical) image processing, segmentation of images is typically performed via U-Nets [1,2].\n",
        "\n",
        "A U-Net consists of an encoder - a series of convolution and pooling layers which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
        "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling.\n",
        "\n",
        "<img src='https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png' width=\"400\"/>\n",
        "\n",
        "\n",
        "At this [link](https://rpubs.com/eR_ic/unet), you find an R implementation of basic U-Nets. At this [link](https://github.com/zhixuhao/unet), you find a Keras implementation of UNets.  \n",
        "Other implementations of more advanced UNets are also made available in [2] at these links: [UNet++](https://github.com/MrGiovanni/UNetPlusPlus)\n",
        "and by the CellSeg organizers as baseline models: [https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/](https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/)\n",
        "\n",
        "\n",
        "### Project aim\n",
        "\n",
        "The aim of the project is to download the *gray-level* (.tiff or .tif files) cell images from the [CellSeg](https://neurips22-cellseg.grand-challenge.org/dataset/) competition and assess the performance of an UNet or any other Deep model for cell segmentation.\n",
        "We suggest using gray-level images to obtain a model that is better specified on a sub class of images.\n",
        "\n",
        "Students are not restricted to use UNets but may other model is wellcome; e.g., even transformer based model in the [leaderboard](https://neurips22-cellseg.grand-challenge.org/evaluation/testing/leaderboard/) may be tested.\n",
        "Students are free to choose any model, as long as they are able to explain their rationale, architecture, strengths and weaknesses.\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science(), vol 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_28\n",
        "\n",
        "[2] Long, F. Microscopy cell nuclei segmentation with enhanced U-Net. BMC Bioinformatics 21, 8 (2020). https://doi.org/10.1186/s12859-019-3332-1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "9rQQQ0U9AEFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown"
      ],
      "metadata": {
        "id": "5co9oUAYAEhl",
        "outputId": "46bae630-cb16-4ff6-9945-c06b15268bf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.4.26)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import tempfile\n",
        "from typing import Callable, List, Tuple\n",
        "from torchvision import transforms\n",
        "from PIL import Image, UnidentifiedImageError"
      ],
      "metadata": {
        "id": "oILKAGLVahzA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = 'data_train/Training-labeled/images/'\n",
        "TRAIN_LABELS_PATH = 'data_train/Training-labeled/labels/'\n",
        "\n",
        "TEST_PATH = 'data_test/Testing/Public/images/'\n",
        "TEST_LABELS_PATH = 'data_test/Testing/Public/labels/'\n",
        "\n",
        "VAL_PATH = 'data_val/Tuning/images/'\n",
        "VAL_LABELS_PATH = ' data_val/Tuning/labels/'\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  dataloader_kwargs = {\"batch_size\": 32, \"shuffle\": True, \"pin_memory\": True}\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  dataloader_kwargs = {\"batch_size\": 64}\n"
      ],
      "metadata": {
        "id": "Cty3s4rIADSz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "[Browse the data](https://drive.google.com/drive/folders/1MaJibsHYitCPOltxVzYjr3rm5s9Vpjpv)"
      ],
      "metadata": {
        "id": "f-UF_9PTAjHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o data_test.zip https://zenodo.org/records/10719375/files/Testing.zip?download=1\n",
        "!unzip -q -d data_test data_test.zip"
      ],
      "metadata": {
        "id": "zdHuhOqEAlle",
        "outputId": "6759dc34-f6f8-4126-bab0-55595e3e3fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2793M  100 2793M    0     0  20.0M      0  0:02:19  0:02:19 --:--:-- 20.0M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o data_train.zip https://zenodo.org/records/10719375/files/Training-labeled.zip?download=1\n",
        "!unzip -q -d data_train data_train.zip"
      ],
      "metadata": {
        "id": "Rx_OhfAwCoG-",
        "outputId": "16ab59a8-09a7-4823-e42a-608a6b115ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1926M  100 1926M    0     0  18.3M      0  0:01:45  0:01:45 --:--:-- 18.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -o data_val.zip https://zenodo.org/records/10719375/files/Tuning.zip?download=1\n",
        "!unzip -q -d data_val data_val.zip"
      ],
      "metadata": {
        "id": "2P-POZuxCo9w",
        "outputId": "4ac72ec9-1790-42dc-d635-60f44c8d0c28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  595M  100  595M    0     0  14.0M      0  0:00:42  0:00:42 --:--:-- 9528k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Partially adapted from https://colab.research.google.com/github/mim-ml-teaching/public-dnn-2024-25/blob/master/docs/DNN-Lab-7-UNet-in-Pytorch-student-version.ipynb\n",
        "class ImageTiffDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,\n",
        "               image_dir: str,\n",
        "               target_dir: str,\n",
        "               cache_dir: str,\n",
        "               filenames: List[str],\n",
        "               transform: torch.nn.Module = transforms.ToTensor(),\n",
        "               target_transform: torch.nn.Module = transforms.ToTensor()):\n",
        "    self.image_dir = image_dir\n",
        "    self.target_dir = target_dir\n",
        "    self.cache_dir = cache_dir\n",
        "    self.filenames = filenames\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "    if not os.path.exists(self.cache_dir):\n",
        "      os.mkdir(self.cache_dir)\n",
        "      os.mkdir(os.path.join(self.cache_dir, \"images\"))\n",
        "      os.mkdir(os.path.join(self.cache_dir, \"target\"))\n",
        "\n",
        "  def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    img_filename = self.filenames[idx]\n",
        "    target_filename = os.path.basename(img_filename)\n",
        "    img_path = os.path.join(self.image_dir, img_filename)\n",
        "    target_path = os.path.join(self.target_dir, target_filename)\n",
        "    img_cache = os.path.join(self.cache_dir, img_filename)\n",
        "    target_cache = os.path.join(self.cache_dir, target_filename)\n",
        "\n",
        "    if not os.path.exists(img_cache):\n",
        "      with Image.open(img_path) as im:\n",
        "        img = self.transform(im)\n",
        "        torch.save(img, img_cache)\n",
        "    else:\n",
        "      img = torch.load(img_cache)\n",
        "\n",
        "    if not os.path.exists(target_cache):\n",
        "      with Image.open(target_path) as im:\n",
        "        target = self.target_transform(im)\n",
        "        torch.save(target, target_cache)\n",
        "    else:\n",
        "      target = torch.load(target_cache)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.filenames)\n",
        "\n",
        "def make_tiff_dataset(image_dir: str, target_dir: str, cache_dir: str):\n",
        "  filenames = []\n",
        "  with os.scandir(image_dir) as it:\n",
        "    for entry in it:\n",
        "      if not entry.is_file():\n",
        "        continue\n",
        "      lower_name = entry.name.lower()\n",
        "      if lower_name.endswith('.tiff') or lower_name.endswith('.tif'):\n",
        "        img_path = entry.path\n",
        "        # Attempt to open the image to check validity\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                # Using .verify() attempts to read the image data and raises\n",
        "                # an error if the file is corrupt or not a valid image format\n",
        "                im.verify()\n",
        "            filenames.append(entry.name)\n",
        "        except (UnidentifiedImageError, IOError) as e:\n",
        "            # Catch specific errors related to image opening/identification\n",
        "            print(f\"Warning: Skipping invalid image file {img_path}: {e}\")\n",
        "        except Exception as e:\n",
        "            # Catch any other unexpected errors during opening/verification\n",
        "            print(f\"Warning: An unexpected error occurred processing {img_path}: {e}\")\n",
        "  return ImageTiffDataset(image_dir, target_dir, cache_dir, filenames)"
      ],
      "metadata": {
        "id": "zHtJrfY_PFRH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = make_tiff_dataset(TRAIN_PATH, TRAIN_LABELS_PATH, tempfile.mkdtemp())\n",
        "test_dataset = make_tiff_dataset(TEST_PATH, TEST_LABELS_PATH, tempfile.mkdtemp())\n",
        "val_dataset = make_tiff_dataset(VAL_PATH, VAL_LABELS_PATH, tempfile.mkdtemp())\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, **dataloader_kwargs)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, **dataloader_kwargs)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, **dataloader_kwargs)\n",
        "\n",
        "print('Train:', len(train_dataset))\n",
        "print('Test:', len(test_dataset))\n",
        "print('Val:', len(val_dataset))"
      ],
      "metadata": {
        "id": "7EN25LfWqtzB",
        "outputId": "b0b66be6-4736-486c-d40a-27bbcccce29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00311.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00311.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00302.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00302.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00313.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00313.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00301.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00301.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00306.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00306.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00304.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00304.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00312.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00312.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00314.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00314.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00505.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00505.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00303.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00303.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00309.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00309.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00507.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00507.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00504.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00504.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00305.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00305.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00308.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00308.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00310.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00310.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00506.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00506.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00315.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00315.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00501.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00501.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00307.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00307.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00502.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00502.tif'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00316.tiff: cannot identify image file 'data_train/Training-labeled/images/cell_00316.tiff'\n",
            "Warning: Skipping invalid image file data_train/Training-labeled/images/cell_00503.tif: cannot identify image file 'data_train/Training-labeled/images/cell_00503.tif'\n",
            "Train: 468\n",
            "Test: 30\n",
            "Val: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic U-Nets"
      ],
      "metadata": {
        "id": "ypzsKKSK-sMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_unet(\n",
        "    model: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    epoch: int,\n",
        "    log_interval: int):\n",
        "  model.train()\n",
        "  correct = 0\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    log_probs = F.log_softmax(output, dim=1)\n",
        "    loss = F.nll_loss(log_probs, target)\n",
        "    pred = log_probs.argmax(\n",
        "        dim=1, keepdim=True\n",
        "    )  # get the index of the max log-probability\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      _, _, image_width, image_height = data.size()\n",
        "      print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "          epoch,\n",
        "          batch_idx * len(data),\n",
        "          len(train_loader.dataset),\n",
        "          100.0 * batch_idx / len(train_loader),\n",
        "          loss.item(),\n",
        "        ))\n",
        "  print(\n",
        "    \"Train accuracy: {}/{} ({:.0f}%)\".format(\n",
        "        correct,\n",
        "          (len(train_loader.dataset) * image_width * image_height),\n",
        "          100.0 * correct / (len(train_loader.dataset) * image_width * image_height),\n",
        "      )\n",
        "  )"
      ],
      "metadata": {
        "id": "I2CL1U3VAcUa"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            dilation=0,\n",
        "            padding_mode='reflect'),\n",
        "        nn.ReLU() # Leaky ReLU?\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class UNetEncoderBlock(nn.Module):\n",
        "  def __init__(self, in_channels: int, out_channels: int, maxpool: bool = True):\n",
        "    super().__init__()\n",
        "    assert out_channels > in_channels\n",
        "    if maxpool:\n",
        "      self.layer = nn.Sequential(\n",
        "          UNetConvBlock(in_channels, out_channels),\n",
        "          UNetConvBlock(out_channels, out_channels),\n",
        "          nn.MaxPool2d(2, dilation=0)\n",
        "      )\n",
        "    else:\n",
        "      self.layer = nn.Sequential(\n",
        "          UNetConvBlock(in_channels, out_channels),\n",
        "          UNetConvBlock(out_channels, out_channels)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer(x)\n",
        "\n",
        "class UNetDecoderBlock(nn.Module):\n",
        "  def __init__(self, in_channels: int, out_channels: int, unmaxpool: bool = True):\n",
        "    super().__init__()\n",
        "    if unmaxpool:\n",
        "      assert out_channels < in_channels\n",
        "      self.layer = nn.Sequential(\n",
        "          nn.ConvTranspose2d(\n",
        "              in_channels + out_channels,\n",
        "              out_channels,\n",
        "              kernel_size=3,\n",
        "              output_padding=1,\n",
        "              dilation=0),\n",
        "          UNetConvBlock(out_channels, out_channels),\n",
        "          UNetConvBlock(out_channels, out_channels)\n",
        "      )\n",
        "    else:\n",
        "      assert False\n",
        "      # assert out_channels == in_channels\n",
        "      # self.layer = nn.Sequential(\n",
        "      #     UNetConvBlock(in_channels, out_channels),\n",
        "      #     UNetConvBlock(out_channels, out_channels)\n",
        "      # )\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, encoder_channels: List[int], decoder_channels: List[int]):\n",
        "    super().__init__()\n",
        "    assert len(encoder_channels) > 0\n",
        "    self.encoder = nn.ModuleList()\n",
        "    self.decoder = nn.ModuleList()\n",
        "\n",
        "    in_channels = 1\n",
        "    for out_channels in encoder_channels[:-1]:\n",
        "      # print(f\"Encoder {in_channels} {out_channels}\")\n",
        "      self.encoder.append(UNetEncoderBlock(in_channels, out_channels))\n",
        "      in_channels = out_channels\n",
        "    self.encoder.append(UNetEncoderBlock(in_channels, encoder_channels[-1], maxpool=False))\n",
        "\n",
        "    in_channels = encoder_channels[-1]\n",
        "    for out_channels in decoder_channels:\n",
        "      # print(f\"Decoder {in_channels} {out_channels}\")\n",
        "      self.decoder.append(UNetDecoderBlock(in_channels, out_channels))\n",
        "      in_channels = out_channels\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = []\n",
        "    for layer in self.encoder:\n",
        "      x = layer(x)\n",
        "      encoded.append(x)\n",
        "    for residual, layer in zip(encoded, self.decoder):\n",
        "      cc = torch.cat((residual, x), dim=1)\n",
        "      x = layer(x)"
      ],
      "metadata": {
        "id": "lrGoupPPhOg6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_unet = UNet(\n",
        "    encoder_channels=[64, 128, 256, 512, 1024],\n",
        "    decoder_channels=[512, 256, 128, 64, 2])\n",
        "\n",
        "optimizer_unet = optim.AdamW(basic_unet.parameters())\n",
        "train_unet(basic_unet, device, train_dataloader, optimizer_unet, 10, 10)"
      ],
      "metadata": {
        "id": "HFk2uaRyVWT0",
        "outputId": "29458208-020d-488c-b246-525284306476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [1, 1608, 1608] at entry 0 and [1, 502, 500] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-eeb3bc02d297>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer_unet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_unet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_unet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-78ce53e8cc46>\u001b[0m in \u001b[0;36mtrain_unet\u001b[0;34m(model, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 1608, 1608] at entry 0 and [1, 502, 500] at entry 1"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}