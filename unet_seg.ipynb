{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wps0/deep4life/blob/main/unet_seg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ82esH_1lEq"
      },
      "source": [
        "## Project : Cell image segmentation projects\n",
        "\n",
        "Contact: Elena Casiraghi (University Milano elena.casiraghi@unimi.it)\n",
        "\n",
        "Cell segmentation is usually the first step for downstream single-cell analysis in microscopy image-based biology and biomedical research. Deep learning has been widely used for cell-image segmentation.\n",
        "The CellSeg competition aims to benchmark cell segmentation methods that could be applied to various microscopy images across multiple imaging platforms and tissue types for cell Segmentation. The  Dataset challenge organizers provide both labeled images and unlabeled ones.\n",
        "The “2018 Data Science Bowl” Kaggle competition provides cell images and their masks for training cell/nuclei segmentation models.\n",
        "\n",
        "In 2022 another [Cell Segmentation challenge was proposed at Neurips](https://neurips22-cellseg.grand-challenge.org/).\n",
        "For interested readers, the competition proceeding has been published on [PMLR](https://proceedings.mlr.press/v212/)\n",
        "\n",
        "### Project Description\n",
        "\n",
        "In the field of (bio-medical) image processing, segmentation of images is typically performed via U-Nets [1,2].\n",
        "\n",
        "A U-Net consists of an encoder - a series of convolution and pooling layers which reduce the spatial resolution of the input, followed by a decoder - a series of transposed convolution and upsampling layers which increase the spatial resolution of the input. The encoder and decoder are connected by a bottleneck layer which is responsible for reducing the number of channels in the input.\n",
        "The key innovation of U-Net is the addition of skip connections that connect the contracting path to the corresponding layers in the expanding path, allowing the network to recover fine-grained details lost during downsampling.\n",
        "\n",
        "<img src='https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-07_at_9.08.00_PM_rpNArED.png' width=\"400\"/>\n",
        "\n",
        "\n",
        "At this [link](https://rpubs.com/eR_ic/unet), you find an R implementation of basic U-Nets. At this [link](https://github.com/zhixuhao/unet), you find a Keras implementation of UNets.  \n",
        "Other implementations of more advanced UNets are also made available in [2] at these links: [UNet++](https://github.com/MrGiovanni/UNetPlusPlus)\n",
        "and by the CellSeg organizers as baseline models: [https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/](https://neurips22-cellseg.grand-challenge.org/baseline-and-tutorial/)\n",
        "\n",
        "\n",
        "### Project aim\n",
        "\n",
        "The aim of the project is to download the *gray-level* (.tiff or .tif files) cell images from the [CellSeg](https://neurips22-cellseg.grand-challenge.org/dataset/) competition and assess the performance of an UNet or any other Deep model for cell segmentation.\n",
        "We suggest using gray-level images to obtain a model that is better specified on a sub class of images.\n",
        "\n",
        "Students are not restricted to use UNets but may other model is wellcome; e.g., even transformer based model in the [leaderboard](https://neurips22-cellseg.grand-challenge.org/evaluation/testing/leaderboard/) may be tested.\n",
        "Students are free to choose any model, as long as they are able to explain their rationale, architecture, strengths and weaknesses.\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "[1] Ronneberger, O., Fischer, P., Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015. MICCAI 2015. Lecture Notes in Computer Science(), vol 9351. Springer, Cham. https://doi.org/10.1007/978-3-319-24574-4_28\n",
        "\n",
        "[2] Long, F. Microscopy cell nuclei segmentation with enhanced U-Net. BMC Bioinformatics 21, 8 (2020). https://doi.org/10.1186/s12859-019-3332-1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rQQQ0U9AEFS"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5co9oUAYAEhl",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gdown\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install opencv-contrib-python\n",
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oILKAGLVahzA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import tempfile\n",
        "from typing import Callable, List, Tuple\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cty3s4rIADSz"
      },
      "outputs": [],
      "source": [
        "TRAIN_PATH = 'data_train/Training-labeled/images/'\n",
        "TRAIN_LABELS_PATH = 'data_train/Training-labeled/labels/'\n",
        "\n",
        "TEST_PATH = 'data_test/Testing/Public/images/'\n",
        "TEST_LABELS_PATH = 'data_test/Testing/Public/labels/'\n",
        "\n",
        "VAL_PATH = 'data_val/Tuning/images/'\n",
        "VAL_LABELS_PATH = 'data_val/Tuning/labels/'\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "  device = torch.device(\"cuda\")\n",
        "  dataloader_kwargs = {\"batch_size\": 2, \"shuffle\": True, \"pin_memory\": True}\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  dataloader_kwargs = {\"batch_size\": 2}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-UF_9PTAjHq"
      },
      "source": [
        "### Data preparation\n",
        "[Browse the data](https://drive.google.com/drive/folders/1MaJibsHYitCPOltxVzYjr3rm5s9Vpjpv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdHuhOqEAlle",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!curl -o data_test.zip \"https://zenodo.org/records/10719375/files/Testing.zip?download=1\"\n",
        "!unzip -d data_test data_test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx_OhfAwCoG-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!curl -o data_train.zip \"https://zenodo.org/records/10719375/files/Training-labeled.zip?download=1\"\n",
        "!unzip -d data_train data_train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2P-POZuxCo9w",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!curl -o data_val.zip \"https://zenodo.org/records/10719375/files/Tuning.zip?download=1\"\n",
        "!unzip -d data_val data_val.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zHtJrfY_PFRH"
      },
      "outputs": [],
      "source": [
        "class PadToSquare:\n",
        "    def __init__(self, size=512, fill=0):\n",
        "        self.size = size\n",
        "        self.fill = fill\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Get current dimensions\n",
        "        w, h = img.size\n",
        "\n",
        "        # Calculate padding\n",
        "        max_dim = max(w, h)\n",
        "        pad_w = (max_dim - w) // 2\n",
        "        pad_h = (max_dim - h) // 2\n",
        "\n",
        "        # Apply padding to make square\n",
        "        padding = (pad_w, pad_h, max_dim - w - pad_w, max_dim - h - pad_h)\n",
        "        img = transforms.functional.pad(img, padding, fill=self.fill)\n",
        "\n",
        "        # Resize to target size\n",
        "        img = transforms.functional.resize(img, (self.size, self.size))\n",
        "        return img\n",
        "\n",
        "\n",
        "# Partially adapted from https://colab.research.google.com/github/mim-ml-teaching/public-dnn-2024-25/blob/master/docs/DNN-Lab-7-UNet-in-Pytorch-student-version.ipynb\n",
        "class ImageTiffDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,\n",
        "               image_dir: str,\n",
        "               target_dir: str,\n",
        "               cache_dir: str,\n",
        "               file_pairs: List[Tuple[str, str]],\n",
        "               transform: torch.nn.Module = transforms.ToTensor(),\n",
        "               target_transform: torch.nn.Module = transforms.ToTensor()):\n",
        "    self.image_dir = image_dir\n",
        "    self.target_dir = target_dir\n",
        "    self.cache_dir = cache_dir\n",
        "    self.file_pairs = file_pairs\n",
        "    self.transform = transforms.Compose([\n",
        "        PadToSquare(size=512),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    self.target_transform = transforms.Compose([\n",
        "        PadToSquare(size=512, fill=0),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    if not os.path.exists(self.cache_dir):\n",
        "      os.mkdir(self.cache_dir)\n",
        "      os.mkdir(os.path.join(self.cache_dir, \"images\"))\n",
        "      os.mkdir(os.path.join(self.cache_dir, \"target\"))\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    img_filename, target_filename = self.file_pairs[idx]\n",
        "\n",
        "    img_path = os.path.join(self.image_dir, img_filename)\n",
        "    target_path = os.path.join(self.target_dir, target_filename)\n",
        "    img_cache = os.path.join(self.cache_dir, img_filename)\n",
        "    target_cache = os.path.join(self.cache_dir, target_filename)\n",
        "\n",
        "    if not os.path.exists(img_cache):\n",
        "      # Change. I had this error: AttributeError: module 'PIL.Image' has no attribute 'load'\n",
        "      #img = Image.load(img_path)\n",
        "      # End of change\n",
        "      img = Image.open(img_path)\n",
        "      img = self.transform(img)\n",
        "      torch.save(img, img_cache)\n",
        "    else:\n",
        "      img = torch.load(img_cache)\n",
        "\n",
        "    if not os.path.exists(target_cache):\n",
        "      # Change. I had this error: AttributeError: module 'PIL.Image' has no attribute 'load'\n",
        "      #target = Image.load(target_path)\n",
        "      target = Image.open(target_path)\n",
        "      # End of change\n",
        "      target = self.target_transform(target)\n",
        "      torch.save(target, target_cache)\n",
        "    else:\n",
        "      target = torch.load(target_cache)\n",
        "\n",
        "    return img, target\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.file_pairs)\n",
        "\n",
        "def is_valid_tiff(path):\n",
        "    try:\n",
        "        with Image.open(path) as img:\n",
        "            img.verify()\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def find_image_mask_pairs(images_dir, masks_dir):\n",
        "    print(f\"Searching in {images_dir} and {masks_dir}\")\n",
        "    image_files = [f for f in os.listdir(images_dir) if f.endswith('.tiff') or f.endswith('.tif')]\n",
        "    mask_files = set(f for f in os.listdir(masks_dir) if f.endswith('.tiff') or f.endswith('.tif'))\n",
        "\n",
        "    pairs = []\n",
        "    for img_file in image_files:\n",
        "        if (len(pairs) > 100): break\n",
        "        base_name = img_file[:-5] if img_file.endswith('.tiff') else img_file[:-4]\n",
        "\n",
        "        found = False\n",
        "        for suffix in ['_label.tiff', '_label.tif']:\n",
        "            candidate_mask = base_name + suffix\n",
        "            if candidate_mask in mask_files:\n",
        "                img_path = os.path.join(images_dir, img_file)\n",
        "                mask_path = os.path.join(masks_dir, candidate_mask)\n",
        "                if is_valid_tiff(img_path) and is_valid_tiff(mask_path):\n",
        "                    pairs.append((img_file, candidate_mask))\n",
        "                else:\n",
        "                    print(f\"Invalid file: {img_file} or {candidate_mask}\")\n",
        "                found = True\n",
        "                break\n",
        "\n",
        "        if not found:\n",
        "            print(f\"No mask for image {img_file}\")\n",
        "\n",
        "    return pairs\n",
        "\n",
        "def make_tiff_dataset(image_dir: str, target_dir: str, cache_dir: str):\n",
        "    file_pairs = find_image_mask_pairs(image_dir, target_dir)\n",
        "    return ImageTiffDataset(image_dir, target_dir, cache_dir, file_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EN25LfWqtzB",
        "outputId": "426e34cd-1de5-4bf5-d0b9-2776810690e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching in data_train/Training-labeled/images/ and data_train/Training-labeled/labels/\n",
            "Invalid file: cell_00502.tif or cell_00502_label.tiff\n",
            "Invalid file: cell_00315.tiff or cell_00315_label.tiff\n",
            "Invalid file: cell_00507.tif or cell_00507_label.tiff\n",
            "Invalid file: cell_00301.tiff or cell_00301_label.tiff\n",
            "Invalid file: cell_00304.tiff or cell_00304_label.tiff\n",
            "Searching in data_test/Testing/Public/images/ and data_test/Testing/Public/labels/\n",
            "Searching in data_val/Tuning/images/ and data_val/Tuning/labels/\n",
            "Train: 101\n",
            "Test: 30\n",
            "Val: 58\n"
          ]
        }
      ],
      "source": [
        "train_dataset = make_tiff_dataset(TRAIN_PATH, TRAIN_LABELS_PATH, tempfile.mkdtemp())\n",
        "test_dataset = make_tiff_dataset(TEST_PATH, TEST_LABELS_PATH, tempfile.mkdtemp())\n",
        "val_dataset = make_tiff_dataset(VAL_PATH, VAL_LABELS_PATH, tempfile.mkdtemp())\n",
        "dataloader_train = torch.utils.data.DataLoader(train_dataset, **dataloader_kwargs)\n",
        "dataloader_test = torch.utils.data.DataLoader(test_dataset, **dataloader_kwargs)\n",
        "dataloader_val = torch.utils.data.DataLoader(val_dataset, **dataloader_kwargs)\n",
        "\n",
        "print('Train:', len(train_dataset))\n",
        "print('Test:', len(test_dataset))\n",
        "print('Val:', len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzsKKSK-sMH"
      },
      "source": [
        "## Basic U-Nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lrGoupPPhOg6"
      },
      "outputs": [],
      "source": [
        "class UNetConvBlock(nn.Module):\n",
        " def __init__(self, in_channels, out_channels):\n",
        "   self.layer = nn.Sequential(\n",
        "       nn.Conv2d(\n",
        "           in_channels,\n",
        "           out_channels,\n",
        "           kernel_size=3,\n",
        "           padding=1,\n",
        "           dilation=0,\n",
        "           padding_mode='reflect'),\n",
        "       nn.ReLU() # Leaky ReLU?\n",
        "   )\n",
        "\n",
        " def forward(self, x):\n",
        "   return self.layer(x)\n",
        "\n",
        "class UNetEncoderBlock(nn.Module):\n",
        " def __init__(self, in_channels: int, out_channels: int, maxpool: bool = True):\n",
        "   assert out_channels > in_channels\n",
        "   if maxpool:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels),\n",
        "         nn.MaxPool2d(2, dilation=0)\n",
        "     )\n",
        "   else:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels)\n",
        "     )\n",
        "\n",
        " def forward(self, x):\n",
        "   return self.layer(x)\n",
        "\n",
        "class UNetDecoderBlock(nn.Module):\n",
        " def __init__(self, in_channels: int, out_channels: int, unmaxpool: bool = True):\n",
        "   assert out_channels < in_channels\n",
        "\n",
        "   if unmaxpool:\n",
        "     assert False\n",
        "     self.layer = nn.Sequential(\n",
        "          # TODO\n",
        "     )\n",
        "   else:\n",
        "     self.layer = nn.Sequential(\n",
        "         UNetConvBlock(in_channels, out_channels),\n",
        "         UNetConvBlock(out_channels, out_channels)\n",
        "     )\n",
        "\n",
        "class UNet(nn.Module):\n",
        " def __init__(self, encoder_channels: List[int], decoder_channels: List[int]):\n",
        "   assert len(encoder_channels) > 0\n",
        "   self.encoder = nn.ModuleList()\n",
        "   self.decoder = nn.ModuleList()\n",
        "\n",
        "    #Expected:\n",
        "    #UNetEncoderBlock(64, 128),\n",
        "    #UNetEncoderBlock(128, 256),\n",
        "    #UNetEncoderBlock(256, 512),\n",
        "    #UNetEncoderBlock(512, 1024, False)\n",
        "   in_channels = 1\n",
        "   for out_channels in encoder_channels[:-1]:\n",
        "     self.encoder.append(UNetEncoderBlock(in_channels, out_channels))\n",
        "     in_channels = out_channels\n",
        "   self.encoder.append(UNetEncoderBlock(in_channels, encoder_channels[-1], maxpool=False))\n",
        "\n",
        "   for out_channels in decoder_channels[:-1]:\n",
        "     self.decoder.append(in_channels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJZYTlqg9orB",
        "outputId": "c705629f-6a98-4d14-95f7-2ff877bb96dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[43. 42. 41. ... 44. 42. 43.]\n",
            " [43. 40. 40. ... 42. 43. 42.]\n",
            " [43. 41. 40. ... 42. 43. 41.]\n",
            " ...\n",
            " [51. 48. 46. ... 42. 43. 42.]\n",
            " [50. 48. 47. ... 43. 44. 42.]\n",
            " [49. 49. 50. ... 45. 46. 46.]]\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "img_path = \"data_train/Training-labeled/images/cell_00302.tiff\"\n",
        "image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "print(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRyc9tw59tLD"
      },
      "source": [
        "## Basic UNet (UNet_1) from https://rpubs.com/eR_ic/unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp-DsAYo-L4d"
      },
      "outputs": [],
      "source": [
        "# This code implements a U-Net model for semantic segmentation from the paper U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional\n",
        "\n",
        "# Implement the double 3X3 convolution blocks\n",
        "# The original paper did not use padding, but we will use padding to keep the image size the same\n",
        "\n",
        "class double_convolution(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the double convolution block which consists of two 3X3 convolution layers,\n",
        "    each followed by a ReLU activation function.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels): # Initialize the class\n",
        "        super().__init__() # Initialize the parent class\n",
        "\n",
        "        # First 3X3 convolution layer\n",
        "        self.first_cnn = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        # Second 3X3 convolution layer\n",
        "        self.second_cnn = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "    # Pass the input through the double convolution block\n",
        "    def forward(self, x):\n",
        "        x = self.first_cnn(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.act2(self.second_cnn(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Implement the Downsample block that occurs after each double convolution block\n",
        "class down_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the downsample block which consists of a Max Pooling layer with a kernel size of 2.\n",
        "    The Max Pooling layer halves the image size reducing the spatial resolution of the feature maps\n",
        "    while retaining the most important features.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "# Implement the UpSample block that occurs in the decoder part of the network\n",
        "class up_sample(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements the upsample block which consists of a convolution transpose layer with a kernel size of 2.\n",
        "    The convolution transpose layer doubles the image size increasing the spatial resolution of the feature maps\n",
        "    while retaining the learned features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "# Implement the crop and concatenate block that occurs in the decoder part of the network\n",
        "# This block concatenates the output of the upsample block with the output of the corresponding downsample block\n",
        "# The output of the crop and concatenate block is then passed through a double convolution block\n",
        "class crop_and_concatenate_fixed(nn.Module):\n",
        "    \"\"\"Memory-efficient crop and concatenate\"\"\"\n",
        "    def forward(self, upsampled, bypass):\n",
        "        # Use F.interpolate instead of torchvision resize (more memory efficient)\n",
        "        if upsampled.shape[2:] != bypass.shape[2:]:\n",
        "            upsampled = F.interpolate(upsampled, size=bypass.shape[2:],\n",
        "                                    mode='bilinear', align_corners=False)\n",
        "        return torch.cat([upsampled, bypass], dim=1)\n",
        "\n",
        "# m = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "# input = torch.randn(1, 1024, 28, 28)\n",
        "# m(input).shape\n",
        "\n",
        "# m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "# xx = torch.randn(1, 1, 143, 143)\n",
        "# m(xx).shape\n",
        "\n",
        "## Implement the UNet architecture\n",
        "class UNet(nn.Module):\n",
        "    # in_channels: number of channels in the input image\n",
        "    # out_channels: number of channels in the output image\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: convolution blocks followed by downsample blocks\n",
        "        self.down_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, 64), (64, 128), (128, 256), (256, 512)]) # List of downsample blocks\n",
        "\n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck layer\n",
        "        self.bottleneck = double_convolution(in_channels = 512, out_channels = 1024)\n",
        "\n",
        "        # Define the expanding path: upsample blocks followed by convolution blocks\n",
        "        self.up_samples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of upsample blocks\n",
        "\n",
        "        self.concat = nn.ModuleList(crop_and_concatenate_fixed() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(double_convolution(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                        [(1024, 512), (512, 256), (256, 128), (128, 64)]) # List of convolution blocks\n",
        "\n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = 64, out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the UNet architecture\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the contacting path\n",
        "        skip_connections = [] # List to store the outputs of the downsample blocks\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            x = down_sample(x)\n",
        "\n",
        "        # Pass the output of the contacting path through the bottleneck layer\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Pass the output of the bottleneck layer through the expanding path\n",
        "        skip_connections = skip_connections[::-1] # Reverse the list of skip connections\n",
        "        for up_sample, concat, up_conv in zip(self.up_samples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, skip_connections.pop(0)) # Remove the first element from the list of skip connections\n",
        "            x = up_conv(x)\n",
        "\n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "# Sanity check for the model\n",
        "import torchsummary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNet(in_channels = 3, out_channels = 1).to(device)\n",
        "dummy_input = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(dummy_input)\n",
        "mask.shape\n",
        "\n",
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWPypoOLK6jd"
      },
      "source": [
        "## Training UNet_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjDQFqN_LHWz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_dice(pred, target):\n",
        "    \"\"\"Calculate Dice Similarity Coefficient with proper edge case handling\"\"\"\n",
        "    # Ensure both are binary\n",
        "    pred = pred.long()\n",
        "    target = target.long()\n",
        "\n",
        "    # Calculate intersection and sums\n",
        "    intersection = (pred & target).float().sum()\n",
        "    pred_sum = pred.float().sum()\n",
        "    target_sum = target.float().sum()\n",
        "\n",
        "    # Handle edge cases:\n",
        "    # 1. Both masks empty (all background) - this is a perfect prediction\n",
        "    if pred_sum == 0 and target_sum == 0:\n",
        "        return 1.0\n",
        "\n",
        "    # 2. One mask empty, other has positives - worst case\n",
        "    if pred_sum == 0 or target_sum == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # 3. Normal case - both have positive pixels\n",
        "    dice = (2.0 * intersection) / (pred_sum + target_sum)\n",
        "\n",
        "    return dice.item()\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total_dice = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Track different types of samples\n",
        "    samples_both_empty = 0\n",
        "    samples_target_empty = 0\n",
        "    samples_pred_empty = 0\n",
        "    samples_both_positive = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = F.binary_cross_entropy_with_logits(output.squeeze(1), target.squeeze(1).float())\n",
        "\n",
        "        # Get predictions\n",
        "        pred = (torch.sigmoid(output) > 0.5).squeeze(1)\n",
        "        target_binary = target.squeeze(1)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Calculate Dice for each image in the batch\n",
        "        batch_dice = 0\n",
        "        for i in range(pred.shape[0]):\n",
        "            dice = calculate_dice(pred[i], target_binary[i])\n",
        "            batch_dice += dice\n",
        "\n",
        "            # Track sample types\n",
        "            pred_positive = pred[i].sum().item() > 0\n",
        "            target_positive = target_binary[i].sum().item() > 0\n",
        "\n",
        "            if not pred_positive and not target_positive:\n",
        "                samples_both_empty += 1\n",
        "            elif not pred_positive and target_positive:\n",
        "                samples_pred_empty += 1\n",
        "            elif pred_positive and not target_positive:\n",
        "                samples_target_empty += 1\n",
        "            else:\n",
        "                samples_both_positive += 1\n",
        "\n",
        "        total_dice += batch_dice\n",
        "        num_samples += pred.shape[0]\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            current_avg_dice = batch_dice / pred.shape[0]\n",
        "            pred_positives = pred.sum().item()\n",
        "            target_positives = target_binary.sum().item()\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}, '\n",
        "                  f'Batch Dice: {current_avg_dice:.4f}, '\n",
        "                  f'Pred+: {pred_positives}, Target+: {target_positives}')\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_dice = total_dice / num_samples\n",
        "\n",
        "    print(f'\\nTrain Epoch: {epoch} Summary:')\n",
        "    print(f'  Average loss: {avg_loss:.4f}, Average Dice: {avg_dice:.4f}')\n",
        "    print(f'  Sample distribution:')\n",
        "    print(f'    Both empty (background only): {samples_both_empty} ({100*samples_both_empty/num_samples:.1f}%)')\n",
        "    print(f'    Target has object, pred empty: {samples_pred_empty} ({100*samples_pred_empty/num_samples:.1f}%)')\n",
        "    print(f'    Target empty, pred has false positives: {samples_target_empty} ({100*samples_target_empty/num_samples:.1f}%)')\n",
        "    print(f'    Both have positives: {samples_both_positive} ({100*samples_both_positive/num_samples:.1f}%)')\n",
        "\n",
        "    return avg_loss, avg_dice\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    total_dice = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Track different types of samples\n",
        "    samples_both_empty = 0\n",
        "    samples_target_empty = 0\n",
        "    samples_pred_empty = 0\n",
        "    samples_both_positive = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = F.binary_cross_entropy_with_logits(\n",
        "                output.squeeze(1),\n",
        "                target.squeeze(1).float(),\n",
        "                reduction='mean'\n",
        "            )\n",
        "\n",
        "            # Get predictions\n",
        "            pred = (torch.sigmoid(output) > 0.5).squeeze(1)\n",
        "            target_binary = target.squeeze(1)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate Dice for each image in the batch\n",
        "            for i in range(pred.shape[0]):\n",
        "                dice = calculate_dice(pred[i], target_binary[i])\n",
        "                total_dice += dice\n",
        "\n",
        "                # Track sample types\n",
        "                pred_positive = pred[i].sum().item() > 0\n",
        "                target_positive = target_binary[i].sum().item() > 0\n",
        "\n",
        "                if not pred_positive and not target_positive:\n",
        "                    samples_both_empty += 1\n",
        "                elif not pred_positive and target_positive:\n",
        "                    samples_pred_empty += 1\n",
        "                elif pred_positive and not target_positive:\n",
        "                    samples_target_empty += 1\n",
        "                else:\n",
        "                    samples_both_positive += 1\n",
        "\n",
        "            num_samples += pred.shape[0]\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    avg_dice = total_dice / num_samples\n",
        "\n",
        "    print(f'\\nTest set Results:')\n",
        "    print(f'  Average loss: {avg_loss:.4f}, Average Dice: {avg_dice:.4f}')\n",
        "    print(f'  Sample distribution:')\n",
        "    print(f'    Both empty (background only): {samples_both_empty} ({100*samples_both_empty/num_samples:.1f}%)')\n",
        "    print(f'    Target has object, pred empty: {samples_pred_empty} ({100*samples_pred_empty/num_samples:.1f}%)')\n",
        "    print(f'    Target empty, pred has false positives: {samples_target_empty} ({100*samples_target_empty/num_samples:.1f}%)')\n",
        "    print(f'    Both have positives: {samples_both_positive} ({100*samples_both_positive/num_samples:.1f}%)\\n')\n",
        "\n",
        "    return avg_loss, avg_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlsGYjl_T9Gk"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "test_batch_size = 10\n",
        "epochs = 5\n",
        "lr = 2e-3\n",
        "use_cuda = True\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "test_size = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5ABCRmT0oWlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFwbsldhL41-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# in_channels=3 for RGB images, out_channels=2 for binary segmentation with 2 classes (background vs object)\n",
        "model = UNet(in_channels=1, out_channels=1).to(device)\n",
        "\n",
        "# Use the Adam optimizer with a small learning rate (good starting point for U-Net training)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Set the number of training epochs\n",
        "num_epochs = 5\n",
        "\n",
        "# Training loop over all epochs\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    train(model, device, dataloader_train, optimizer, epoch, log_interval)\n",
        "    test(model, device, dataloader_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AWEoikrlhH8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "path = \"data_train/Training-labeled/images/cell_00311.tiff\"\n",
        "print(f\"File exists: {os.path.exists(path)}\")\n",
        "print(f\"File size: {os.path.getsize(path)} bytes\")\n",
        "\n",
        "import cv2\n",
        "\n",
        "img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "print(type(img), img.shape if img is not None else \"Can't load image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmTklIAGl_yI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def validate_tiff_dataset(image_dir, label_dir):\n",
        "    broken_images = []\n",
        "    broken_labels = []\n",
        "\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    label_files = sorted(os.listdir(label_dir))\n",
        "\n",
        "    print(f\"Sprawdzam {len(image_files)} obrazów i {len(label_files)} masek...\")\n",
        "\n",
        "    for filename in image_files:\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                img.verify()  # tylko weryfikuje (nie ładuje całkowicie)\n",
        "        except (UnidentifiedImageError, OSError, FileNotFoundError) as e:\n",
        "            print(f\"[BŁĄD OBRAZU] {filename}: {e}\")\n",
        "            broken_images.append(filename)\n",
        "\n",
        "    for filename in label_files:\n",
        "        label_path = os.path.join(label_dir, filename)\n",
        "        try:\n",
        "            with Image.open(label_path) as img:\n",
        "                img.verify()\n",
        "        except (UnidentifiedImageError, OSError, FileNotFoundError) as e:\n",
        "            print(f\"[BŁĄD MASKI] {filename}: {e}\")\n",
        "            broken_labels.append(filename)\n",
        "\n",
        "    print(\"\\nPodsumowanie:\")\n",
        "    print(f\"- Uszkodzone obrazy: {len(broken_images)}\")\n",
        "    print(f\"- Uszkodzone maski: {len(broken_labels)}\")\n",
        "\n",
        "    return broken_images, broken_labels\n",
        "\n",
        "bad_imgs, bad_labels = validate_tiff_dataset(TRAIN_PATH, TRAIN_LABELS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "348D8uo9raMa"
      },
      "outputs": [],
      "source": [
        "Image.open('data_train/Training-labeled/images/cell_00501.tif').verify()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD2B-B0NwdlQ"
      },
      "outputs": [],
      "source": [
        "target_np = cv2.imread('data_train/Training-labeled/labels/cell_00490_label.tiff', cv2.IMREAD_UNCHANGED)\n",
        "print(target_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwUzrjwtrMn2"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "for img in bad_imgs:\n",
        "    path = os.path.join(TRAIN_PATH, img)\n",
        "    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "    if image is None:\n",
        "        print(f\"{img} nie da się otworzyć przez OpenCV\")\n",
        "    else:\n",
        "        print(f\"{img} OK — można użyć OpenCV zamiast PIL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNFh-xjVPuVY"
      },
      "source": [
        "## Testing UNet_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANmKIkz-QEjM"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_unet(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)             # (B, C, H, W)\n",
        "        masks = masks.to(device)               # (B, H, W) — ground truth z etykietami klas\n",
        "\n",
        "        outputs = model(images)                # (B, num_classes, H, W)\n",
        "        preds = outputs.argmax(dim=1)          # (B, H, W)\n",
        "\n",
        "        total_correct += (preds == masks).sum().item()\n",
        "        total_pixels += masks.numel()\n",
        "\n",
        "    accuracy = 100.0 * total_correct / total_pixels\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_predictions(model, dataloader, device, num_examples=3):\n",
        "    model.eval()\n",
        "    for images, masks in dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        for i in range(min(num_examples, images.size(0))):\n",
        "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "            mask = masks[i].cpu().numpy()\n",
        "            pred = preds[i].cpu().numpy()\n",
        "\n",
        "            fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
        "            axs[0].imshow(img)\n",
        "            axs[0].set_title(\"Input Image\")\n",
        "            axs[1].imshow(mask)\n",
        "            axs[1].set_title(\"Ground Truth\")\n",
        "            axs[2].imshow(pred)\n",
        "            axs[2].set_title(\"Prediction\")\n",
        "            plt.show()\n",
        "        break  # tylko jedna batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEge6jW3QLLR"
      },
      "outputs": [],
      "source": [
        "test_unet(model, dataloader_test, device)\n",
        "visualize_predictions(model, dataloader_test, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkk-5CVkTSVD"
      },
      "source": [
        "## ResidualAttentionUnet (UNet_2) from https://rpubs.com/eR_ic/unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il9-awnFTyMW"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "# Define a Residual block\n",
        "class residual_block(nn.Module):\n",
        "    \"\"\"\n",
        "    This class implements a residual block which consists of two convolution layers with group normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 8):\n",
        "        super().__init__()\n",
        "        # First convolution layer\n",
        "        self.first_conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.first_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act1 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Second convolution layer\n",
        "        self.second_conv = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = 1)\n",
        "        self.second_norm = nn.GroupNorm(num_groups = n_groups, num_channels = out_channels)\n",
        "        self.act2 = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # If the number of input channels is not equal to the number of output channels,\n",
        "        # then use a 1X1 convolution layer to compensate for the difference in dimensions\n",
        "        # This allows the input to have the same dimensions as the output of the residual block\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1)\n",
        "        else:\n",
        "            # Pass the input as is\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    # Pass the input through the residual block\n",
        "    def forward(self, x):\n",
        "        # Store the input\n",
        "        input = x\n",
        "\n",
        "        # Pass input through the first convolution layer\n",
        "        x = self.act1(self.second_norm(self.first_conv(x)))\n",
        "\n",
        "        # Pass the output of the first convolution layer through the second convolution layer\n",
        "        x = self.act2(self.second_norm(self.second_conv(x)))\n",
        "\n",
        "        # Add the input to the output of the second convolution layer\n",
        "        # This is the skip connection\n",
        "        x = x + self.shortcut(input)\n",
        "        return x\n",
        "\n",
        "# Implement the DownSample block that occurs after each residual block\n",
        "class down_sample(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the downsample block\n",
        "    def forward(self, x):\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "# Implement the UpSample block that occurs in the decoder path/expanding path\n",
        "class up_sample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolution transpose layer to upsample the input\n",
        "        self.up_sample = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "\n",
        "    # Pass the input through the upsample block\n",
        "    def forward(self, x):\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "# Implement the crop and concatenate layer\n",
        "class crop_and_concatenate(nn.Module):\n",
        "    def forward(self, upsampled, bypass):\n",
        "        # Crop the upsampled feature map to match the dimensions of the bypass feature map\n",
        "        upsampled = torchvision.transforms.functional.resize(upsampled, size = bypass.shape[2:], antialias=True)\n",
        "        x = torch.cat([upsampled, bypass], dim = 1) # Concatenate along the channel dimension\n",
        "        return x\n",
        "\n",
        "# Implement an attention block\n",
        "class attention_block(nn.Module):\n",
        "    def __init__(self, skip_channels, gate_channels, inter_channels = None, n_groups = 8):\n",
        "        super().__init__()\n",
        "\n",
        "        if inter_channels is None:\n",
        "            inter_channels = skip_channels // 2\n",
        "\n",
        "        # Implement W_g i.e the convolution layer that operates on the gate signal\n",
        "        # Upsample gate signal to be the same size as the skip connection\n",
        "        self.W_g = up_sample(in_channels = gate_channels, out_channels = skip_channels)\n",
        "        #self.W_g_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "        #self.W_g_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement W_x i.e the convolution layer that operates on the skip connection\n",
        "        self.W_x = nn.Conv2d(in_channels = skip_channels, out_channels = inter_channels, kernel_size = 1)\n",
        "        #self.W_x_norm = nn.GroupNorm(num_groups = n_groups, num_channels = inter_channels)\n",
        "        #self.W_x_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement phi i.e the convolution layer that operates on the output of W_x + W_g\n",
        "        self.phi = nn.Conv2d(in_channels = inter_channels, out_channels = 1, kernel_size = 1)\n",
        "        #self.phi_norm = nn.GroupNorm(num_groups = n_groups, num_channels = 1)\n",
        "        #self.phi_act = nn.SiLU() # Swish activation function\n",
        "\n",
        "        # Implement the sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        # Implement the Swish activation function\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        # Implement final group normalization layer\n",
        "        self.final_norm = nn.GroupNorm(num_groups = n_groups, num_channels = skip_channels)\n",
        "\n",
        "    # Pass the input through the attention block\n",
        "    def forward(self, skip_connection, gate_signal):\n",
        "        # Upsample the gate signal to match the channels of the skip connection\n",
        "        gate_signal = self.W_g(gate_signal)\n",
        "        # Ensure that the sizes of the skip connection and the gate signal match before addition\n",
        "        if gate_signal.shape[2:] != skip_connection.shape[2:]:\n",
        "            gate_signal = torchvision.transforms.functional.resize(gate_signal, size = skip_connection.shape[2:], antialias=True)\n",
        "        # Project to the intermediate channels\n",
        "        gate_signal = self.W_x(gate_signal)\n",
        "\n",
        "        # Project the skip connection to the intermediate channels\n",
        "        skip_signal = self.W_x(skip_connection)\n",
        "\n",
        "        # Add the skip connection and the gate signal\n",
        "        add_xg = gate_signal + skip_signal\n",
        "\n",
        "        # Pass the output of the addition through the activation function\n",
        "        add_xg = self.act(add_xg)\n",
        "\n",
        "        # Pass the output of attention through a 1x1 convolution layer to obtain the attention map\n",
        "        attention_map = self.sigmoid(self.phi(add_xg))\n",
        "\n",
        "        # Multiply the skip connection with the attention map\n",
        "        # Perform element-wise multiplication\n",
        "        skip_connection = torch.mul(skip_connection, attention_map)\n",
        "\n",
        "        skip_connection = nn.Conv2d(in_channels = skip_connection.shape[1], out_channels = skip_connection.shape[1], kernel_size = 1)(skip_connection)\n",
        "        skip_connection = self.act(self.final_norm(skip_connection))\n",
        "\n",
        "        return skip_connection\n",
        "\n",
        "\n",
        "## Implement a residual attention U-Net\n",
        "class ResidualAttentionUnet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_groups = 4, n_channels = [64, 128, 256, 512, 1024]):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define the contracting path: residual blocks followed by downsampling\n",
        "        self.down_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(in_channels, n_channels[0]), (n_channels[0], n_channels[1]), (n_channels[1], n_channels[2]), (n_channels[2], n_channels[3])])\n",
        "        self.down_samples = nn.ModuleList(down_sample() for _ in range(4))\n",
        "\n",
        "        # Define the bottleneck residual block\n",
        "        self.bottleneck = residual_block(n_channels[3], n_channels[4])\n",
        "\n",
        "        # Define the attention blocks\n",
        "        self.attention_blocks = nn.ModuleList(attention_block(skip_channels = residuals_chans, gate_channels = gate_chans) for gate_chans, residuals_chans in\n",
        "                                              [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        # Define the expanding path: upsample blocks, followed by crop and concatenate, followed by residual blocks\n",
        "        self.upsamples = nn.ModuleList(up_sample(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                       [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        self.concat = nn.ModuleList(crop_and_concatenate() for _ in range(4))\n",
        "\n",
        "        self.up_conv = nn.ModuleList(residual_block(in_chans, out_chans) for in_chans, out_chans in\n",
        "                                     [(n_channels[4], n_channels[3]), (n_channels[3], n_channels[2]), (n_channels[2], n_channels[1]), (n_channels[1], n_channels[0])])\n",
        "\n",
        "        # Final 1X1 convolution layer to produce the output segmentation map:\n",
        "        # The primary purpose of 1x1 convolutions is to transform the channel dimension of the feature map,\n",
        "        # while leaving the spatial dimensions unchanged.\n",
        "        self.final_conv = nn.Conv2d(in_channels = n_channels[0] , out_channels = out_channels, kernel_size = 1)\n",
        "\n",
        "    # Pass the input through the residual attention U-Net\n",
        "    def forward(self, x):\n",
        "        # Store the skip connections\n",
        "        skip_connections = []\n",
        "        # # Store the gate signals\n",
        "        # gate_signals = []\n",
        "\n",
        "        # Pass the input through the contracting path\n",
        "        for down_conv, down_sample in zip(self.down_conv, self.down_samples):\n",
        "            x = down_conv(x)\n",
        "            skip_connections.append(x)\n",
        "            #gate_signals.append(x)\n",
        "            x = down_sample(x)\n",
        "\n",
        "        # Pass the output of the contracting path through the bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections.append(x)\n",
        "\n",
        "        # Attention on the residual connections\n",
        "        #skip_connections = skip_connections[::-1]\n",
        "        n = len(skip_connections)\n",
        "        indices = [(n - 1 - i, n - 2 - i) for i in range(n - 1)]\n",
        "        attentions = []\n",
        "        for i, g_x in enumerate(indices):\n",
        "            g_gate = g_x[0]\n",
        "            x_residual = g_x[1]\n",
        "            attn = self.attention_blocks[i](skip_connections[x_residual], skip_connections[g_gate])\n",
        "            attentions.append(attn)\n",
        "\n",
        "        #attentions = attentions[::-1]\n",
        "\n",
        "        # Pass the output of the attention blocks through the expanding path\n",
        "        for up_sample, concat, up_conv in zip(self.upsamples, self.concat, self.up_conv):\n",
        "            x = up_sample(x)\n",
        "            x = concat(x, attentions.pop(0))\n",
        "            x = up_conv(x)\n",
        "\n",
        "        # Pass the output of the expanding path through the final convolution layer\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "## Sanity check\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResidualAttentionUnet(in_channels = 3, out_channels = 1).to(device)\n",
        "x = torch.randn((1, 3, 32, 32)).to(device)\n",
        "mask = model(x)\n",
        "mask.shape\n",
        "\n",
        "# See how data flows through the network\n",
        "torchsummary.summary(model, input_size = (3, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lls_HclWTqwe"
      },
      "source": [
        "## Training UNet_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6NQx9nYTqgT"
      },
      "source": [
        "## Testing UNet_2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ypzsKKSK-sMH"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}